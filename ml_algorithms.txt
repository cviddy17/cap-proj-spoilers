
1.A linear regression is fit on 3 predictors, x1, x2, and x3.
Interpret the beta coefficient corresponding to the 3rd predictor, x3.

x1 and x2 go up and down in the same direction
of the output by a factor of beta1 and beta2.

The third predictor changes in the opposite direction of the output by a factor
or |beta3|.

What do R-squared and Adj. R-squared tell us? Why are they different?

R Squared tells us the overall correlation to the model, Adjust R Squared
takes into account the number of features we have in the model. They are
different because of the y-intercept.


Linear Regression Output


2.
Many of the wells in Bangladesh are contaminated with natural arsenic, a poisonous substance that causes cancer. Any locality can contain wells with various arsenic levels, and any given contaminated well often has a nearby well that is clean. A research team measured all the wells and labeled them as safe of unsafe, and encouraged the families with unsafe wells to switch to a nearby wells that were free of arsenic. Three years later the team revisited, and collected data on which families had switched, and which had not. A logistic regression was fit to estimate the probability that a family switched to a clean well given:

The distance to the closest known clean well (in meters).
The arsenic level of the family's well.
An indicator for whether any family members are active in a community organization.
The education level of the household.
Predictor	Parameter Estiamte	Standard Error
intercept	-0.157	0.100
arsenic_levels	0.467	0.042
distance_to_clean_well	-0.009	0.001
active_in_community	-0.124	0.077
education_level	0.042	0.010
Interpret the coefficient for distance_to_clean_well. Given the column of standard error estimates, are there any coefficients you would be wary about interpreting in this way? (This example is adapted from "Data Analysis Using Regression and Multilevel Models", by Gelman and Hill).

The data does not appear to be standardized, as the other coefficients are more
affected by a change in the output. Education level also does not seem to be
standardized.


3.
What is the Bias-Variance Tradeoff?

a. Explain what "Bias" and "Variance" are.
Bias is the component of the error of the model not explained by the model,
variance is the component of the error of the model due to fitting within the model
(Much easier to explain with pictures.)

b. When we overfit, what happens to bias and variance?
Variance is high, bias is low

c. When we underfit, what happens to bias and variance?
Bias is high, variance is low

d. How do Ridge and Lasso attempt to win at the Bias-Variance tradeoff? What exactly is being penalized?

There is a regularization coefficient to the model that will minimize the overall bias.
When this is parameter is set, Bias is lower.

Bias Variance Test Curve


4
You have a dataset which looks like the scatterplot below. You would like to build a model to classify the red and green datapoints. Name one classification model you think would perform the best and explain why.

SVM with a Gaussian Kernel - By using a kernel that transcends the
dimensionality of the space, The transformation of the model can better
differentiate between the two points. Plus saying 'KNN' would be the easy,
copout answer.

Classification


5
You have a bag full of marbles: 501 blue and 530 red to be exact.

a. What is the Gini impurity of your bag of marbles?

def _gini(self, y):
      '''
      INPUT:
          - y: 1d numpy array
      OUTPUT:
          - float
      Return the gini impurity of the array y.
      '''

      total = 0
      for cl in np.unique(y):
          prob = np.sum(y == cl) / float(len(y))
          total += prob ** 2
      return 1 - total

      1 - (a/(a+b))**2 - (b/(a+b))**2
      Out[3]: 0.4996044069349675


b. You split your bag of marbles into two bags! Bag1 has 1 blue and 30 Red. Bag2 has 500 blue and 500 red. What is the Gini impurity of Bag1 and Bag 2? What is the Information Gain in going from Bag --> Bag1 and Bag2?

c. Consider a different split where Bag1 has 100 blue and 400 red, and Bag2 has 401 blue and 130 red.
Again, use Gini impurity to compute the Information Gain.

d. Why is the gain much better in part (c), despite the purity of Bag1 in part (b)?

e. In the general classification tree context, for any given node, how is a particular split chosen?

Marbles


6
Trees, trees, trees!

a. Why does Bagging almost always outperform a single decision tree?

The aggregation of samples of data will lower the overall bias than
a single tree of the same depth.


b. Why does Random Forest often outperform Bagging?

Random Forests outperform bagging, because a sample of random features are selected,
whereas in traditional bagging, the entire feature set is used.
This prevents local minima issues that may arise from traversing the same
tree over and over again.

c. Boosting can often outperform Random Forest.
In fact, when one looks closely growing boosted trees is quite different from building Random Forests.
Name two ways boosting is different from Random Forests.

Random Forests are parallelizable, since each tree is built off the first tree.
Boosting is iterative and depends on the tree before it.

Random Forests do not require a learning parameter.
Since Boosting algorithms may miss the target, they usually have a learning
algorithm as to not overfit.


7
What is PCA? How can it be used to perform PCR? Describe principal components and scree plots.

If it is helpful, use the mad libs below:

The 1st Principal Component is simply a linear combination of the
feature matrix that produces the largest variance, subject to the constraint that
minimizes Squared Distance to X.
 Similarly, the 2nd principal component...3rd principal component...and so forth
 can be constructed similarly, subject to the additional constraint that They
 minimize to their distance on their axes.

The first k principal components can be thought of as all of your data projected
into a k-feature-dimensional space.

We can tell how much of the VARIANCE in the data is explained by the first k
principal components by examining the eigenvalues.

To perform Principal Components Regression, we can take the eigenvectors
and use them simply as predictors in Linear Regression.



Why is the curse of dimensionality particularly problematic for algorithms like
k-th Nearest Neighbor and k-Means Clustering?

As dimensions increase, distance metrics such as euclidian distance scale
exponentially. Thus distance becomes less interpretable in higher dimensions.


Discuss why Naïve Bayes is so naïve.
It only takes overall word count into effect.


Which of the following methods do you think would necessitate preprocessing
of the data (scaling/standardizing)? For those that require preprocessing,
give a brief description of why.

Linear regression, Logistic Regression, Lasso/Ridge, PCA, k-Means, kNN, SVM,
Decision Tree, Random Forest, Boosting

Lasso/Ridge - Optimizing regularization coefficient better when scaled
Logistic Regression - Allows coefficients to be interpretable with each other
k-Means - Curse of Dimensionality
KNN - Curse of Dimensionality
SVM - Helps to optimize hyperparameter
